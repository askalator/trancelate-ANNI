from fastapi import FastAPI, HTTPException, Response
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import re, os, csv, json, pathlib, urllib.request, time

# -------- Config
BACKEND = os.environ.get("MT_BACKEND","http://127.0.0.1:8090/translate")
PROVIDER = os.environ.get("PROVIDER_URL")
TIMEOUT = int(os.environ.get("MT_TIMEOUT", "60"))
TM_SOFT_THRESHOLD = float(os.environ.get("TM_SOFT_THRESHOLD", "0.90"))

# -------- App
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_methods=["*"],
    allow_headers=["*", "x-anni-key", "X-API-Key", "content-type"]
)

@app.exception_handler(Exception)
async def _err_handler(request, exc):
    import traceback
    return JSONResponse(status_code=500, content={"ok": False, "error": str(exc), "trace": traceback.format_exc()})

# -------- Regexes & helpers
PH_RE         = re.compile(r"\{\{[^}]+\}\}")
SINGLE_PH_RE  = re.compile(r"\{[A-Za-z0-9_]+\}")
NUM_RE        = re.compile(r"\d+[.,]?\d*")
TAG_RE        = re.compile(r"</?([A-Za-z0-9]+)[^>]*>")
TAG_FULL_RE   = re.compile(r"(</?[A-Za-z0-9]+(?:\s[^>]*?)?>)")
AMPM_RE       = re.compile(r"\b([1-9]|1[0-2])\s*(a\.?m\.?|p\.?m\.?)\b", re.I)
RANGE_RE      = re.compile(r"\b\d{1,4}\s*[–-]\s*\d{1,4}\b")
VER_RE        = re.compile(r"\b([A-Za-z][A-Za-z0-9\-\+\.#/]{1,})\s?([0-9]{1,3}(?:\.[0-9]+)?)\b")
PURENUM_RE    = re.compile(r"\d+(?:[.,]\d+)?")
EMOJI_RE      = re.compile('[\u2600-\u27BF\uFE0F\U0001F1E6-\U0001F1FF\U0001F300-\U0001FAFF]')
SYMBOL_RE     = re.compile(r'[®™©℠]')
HASHTAG_RE    = re.compile(r'#[A-Za-z0-9_äöüÄÖÜß\-\._]+')

def has_emoji(t:str)->bool:
    try:
        return bool(EMOJI_RE.search(t or ""))
    except Exception:
        return False

ABBR_PAT = [r"Mr\.", r"Mrs\.", r"Ms\.", r"Dr\.", r"Prof\.", r"Sr\.", r"Jr\.", r"vs\.", r"etc\.", r"e\.g\.", r"i\.e\.", r"U\.S\.", r"U\.K\.", r"Fig\.", r"No\.", r"ca\."]
SPLIT_RE = re.compile(r"(?<=[.!?])\s+(?=[A-Z0-9])")

def mask_abbr(t:str)->str:
    for p in ABBR_PAT: t = re.sub(p, lambda m: m.group(0).replace(".", "§DOT§"), t, flags=re.I)
    return t
def unmask_abbr(t:str)->str: return t.replace("§DOT§",".")

def sentence_split(text:str):
    t = mask_abbr(text)
    parts = [p.strip() for p in SPLIT_RE.split(t)]
    parts = [unmask_abbr(p) for p in parts if p]
    return parts if parts else [text]

def digits_only(s): return re.sub(r"\D","",s or "")
def tags(sig): return [m.group(0).lower().replace(" ", "") for m in TAG_RE.finditer(sig)]

def numbers_ok(src:str, out:str)->bool:
    out_dig = digits_only(out)
    src_nums = NUM_RE.findall(src)
    direct_ok = True
    for n in src_nums:
        n_d = digits_only(n)
        if n_d and n_d not in out_dig: direct_ok = False; break
    if direct_ok: return True
    had_ampm=False
    out_parts = re.findall(r"\d{1,4}", out)
    present = lambda x: x in out_dig or x in out_parts
    for m in AMPM_RE.finditer(src):
        had_ampm=True
        val = int(m.group(1)); mer = m.group(2).lower(); raw = str(val)
        if ('p' in mer) and (1<=val<=11):
            if not (present(raw) or present(str(val+12))): return False
        else:
            if not present(raw): return False
    return had_ampm or direct_ok

def check_invariants(src:str, out:str):
    ph_ok  = all(p in out for p in PH_RE.findall(src))
    num_ok = numbers_ok(src, out)
    html_ok= sorted(tags(src)) == sorted(tags(out))
    paren_ok = True
    src_len=len(src); ratio=(len(out)+1)/(src_len+1)
    len_ok = (0.4 <= ratio <= 4.0) if src_len < 20 else (0.5 <= ratio <= 2.2)
    ok = ph_ok and num_ok and html_ok and paren_ok and len_ok
    return {"ok": ok, "ph_ok": ph_ok, "num_ok": num_ok, "html_ok": html_ok, "paren_ok": paren_ok, "len_ratio": round(ratio,2)}

def cp1252_cleanup(text:str)->str:
    return (text.replace("Â©","©").replace("Â®","®").replace("Â·","·").replace("Â",""))

EN_SENTENCE_CASE_PATTERNS = [
    (re.compile(r'(\{\{[^}]+\}\}\s+)([A-Z][a-z]+\b)'), 1, 2),
    (re.compile(r'(</[^>]+>\s+)([A-Z][a-z]+\b)'), 1, 2),
    (re.compile(r'(–\s+)([A-Z][a-z]+\b)'), 1, 2),
    (re.compile(r'(\d+\s+)([A-Z][a-z]+\b)'), 1, 2),
]
EN_LIKELY = re.compile(r"\b(the|and|you|we|in|on|with|pages?|price|incl|vat|now|send|save|synchronized)\b", re.I)
def fix_sentence_case_en(text:str)->str:
    if not EN_LIKELY.search(text): return text
    prev=None; t=text
    while prev!=t:
        prev=t
        for pat, grp1, grp2 in EN_SENTENCE_CASE_PATTERNS:
            t = pat.sub(lambda m: m.group(grp1) + m.group(grp2).lower(), t)
    return t

def restore_list(text, prefix, items):
    for i, item in enumerate(items):
        text = text.replace(f"⟦{prefix}{i}⟧", item)
    return text

def _dequote_wrapped(t:str)->str:
    import re
    t = re.sub(r"['\"“”]\s*(</?[A-Za-z0-9]+(?:\s[^>]*?)?>)\s*['\"“”]", r"\1", t)
    t = re.sub(r"['\"“”]\s*(\{\{[^}]+\}\})\s*['\"“”]", r"\1", t)
    t = re.sub(r"['\"“”]\s*(\{[A-Za-z0-9_]+\})\s*['\"“”]", r"\1", t)
    t = re.sub(r"['\"“”]\s*([®™©℠])\s*['\"“”]", r"\1", t)
    t = re.sub(r"['\"“”]\s*(#[A-Za-z0-9_ÄÖÜäöüß\.\-]+)\s*['\"“”]", r"\1", t)
    return t

def _collapse_dupe_tags(t:str)->str:
    import re
    prev=None
    while prev!=t:
        prev=t
        t=re.sub(r"<([A-Za-z0-9]+)(?:\s[^>]*)?>\s*<\1(?:\s[^>]*)?>", r"<\1>", t)
        t=re.sub(r"</([A-Za-z0-9]+)>\s*</\1>", r"</\1>", t)
    return t

# -------- TM & Glossary
TM = {}; NEVER_TERMS = set()

def load_tm():
    try:
        with open("tm.csv", "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) >= 4:
                    src_lang = row[0]
                    tgt_lang = row[1] 
                    source_text = row[2]
                    target_text = row[3]
                    
                    key = f"{src_lang}|{tgt_lang}|{source_text}"
                    TM[key] = {
                        "src": source_text, 
                        "tgt": target_text, 
                        "src_lang": src_lang,
                        "tgt_lang": tgt_lang,
                        "tgt_ph_set": set(PH_RE.findall(target_text))
                    }
    except FileNotFoundError:
        pass
    return len(TM)

def load_glossary():
    try:
        with open("glossary.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            NEVER_TERMS.update(data.get("never_translate", []))
    except FileNotFoundError:
        pass
    return len(NEVER_TERMS)

load_tm(); load_glossary()

def tm_lookup_exact(src, tgt, text):
    key = f"{src}|{tgt}|{text}"
    if key in TM:
        return {"tgt": TM[key]["tgt"], "provenance": {"tm": "exact", "engine": "tm"}}
    return None

def tm_lookup_fuzzy(src, tgt, text):
    try:
        from rapidfuzz import fuzz
    except ImportError:
        return None
    
    cands = [v for k,v in TM.items() if k.startswith(f"{src}|{tgt}|")]
    if not cands: return None
    L=len(text); pool=[it for it in cands if 0.8*L<=len(it["src"])<=1.2*L] or cands
    best=None; best_score=0.0; q=text.lower()
    for it in pool:
        s=fuzz.ratio(q, it["src"].lower())/100.0
        if s>best_score: best_score, best = s, it
    if best and best_score>=TM_SOFT_THRESHOLD:
        return {"tgt": best["tgt"], "provenance":{"tm":"fuzzy","engine":"tm","score":round(best_score,3)}}
    return None

# -------- Backend
def call_backend(url, src, tgt, text):
    import json, urllib.request, urllib.error
    data = json.dumps({"source": src, "target": tgt, "text": text}).encode("utf-8")
    req = urllib.request.Request(url, method="POST",
        headers={"Content-Type":"application/json","Connection":"close"}, data=data)
    try:
        with urllib.request.urlopen(req, timeout=TIMEOUT) as r:
            body = r.read().decode("utf-8", "ignore")
    except urllib.error.HTTPError as e:
        try:
            err_body = e.read().decode("utf-8", "ignore")
        except Exception:
            err_body = ""
        raise HTTPException(status_code=502, detail={"backend_status": e.code, "backend_body": err_body})
    except urllib.error.URLError as e:
        raise HTTPException(status_code=502, detail=f"backend_unreachable: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"backend_error: {e}")
    try:
        j = json.loads(body) if body else {}
    except Exception:
        raise HTTPException(status_code=502, detail={"backend_status": "invalid_json", "backend_body": body[:500]})
    return j.get("translated_text","")

def _direct_pair(src, tgt):
    s=(src or '').lower()[:2]; t=(tgt or '').lower()[:2]
    return (s=="en") or (t=="en")

def normalize_colons(t:str)->str:
    t=re.sub(r"(?<=\d)\s*:\s*(?=\d{2}\b)", ":", t)
    t=re.sub(r"(?<!\d)\s*:\s*(?!\d)", ": ", t)
    return t

def _singlecall_masked(src_lang, tgt_lang, text):
    ph_hits=[]; tag_hits=[]; sb_hits=[]; emj_hits=[]; sym_hits=[]; hash_hits=[]
    def r_ph(m):  i=len(ph_hits);  ph_hits.append(m.group(0));   return f"⟦PH{i}⟧"
    def r_tag(m): i=len(tag_hits); tag_hits.append(m.group(0));  return f"⟦TAG{i}⟧"
    def r_sb(m):  i=len(sb_hits);  sb_hits.append(m.group(0));   return f"⟦SB{i}⟧"
    t = PH_RE.sub(r_ph, text)
    t = TAG_FULL_RE.sub(r_tag, t)
    t = re.sub(r"\{[A-Za-z0-9_]+\}", r_sb, t)
    def r_emj(m): i=len(emj_hits); emj_hits.append(m.group(0)); return f"⟦EMJ{i}⟧"
    def r_sym(m): i=len(sym_hits); sym_hits.append(m.group(0)); return f"⟦SYM{i}⟧"
    def r_hash(m): i=len(hash_hits); hash_hits.append(m.group(0)); return f"⟦HASH{i}⟧"
    t = EMOJI_RE.sub(r_emj, t)
    t = SYMBOL_RE.sub(r_sym, t)
    t = HASHTAG_RE.sub(r_hash, t)
    out = call_backend(BACKEND, src_lang, tgt_lang, t)
    out = restore_list(out, "HASH", hash_hits)
    out = restore_list(out, "SYM",  sym_hits)
    out = restore_list(out, "EMJ",  emj_hits)
    out = restore_list(out, "TAG",  tag_hits)
    out = restore_list(out, "SB",   sb_hits)
    out = restore_list(out, "PH",   ph_hits)
    out = _dequote_wrapped(out)
    out = _collapse_dupe_tags(out)
    out = _dequote_wrapped(out)
    out = re.sub(r"\s+([,.;!?])", r"\1", out)
    out = normalize_colons(out)
    out = re.sub(r"\s{2,}", " ", out).strip()
    out = fix_sentence_case_en(out)
    out = cp1252_cleanup(out)
    return out, "self_host_mt"

def _pivot_via_en(src, tgt, text):
    outs=[]
    for sent in sentence_split(text):
        mid, _ = _singlecall_masked(src, "en", sent)
        o, _   = _singlecall_masked("en", tgt, mid)
        outs.append(o)
    return " ".join(outs)

# -------- API
class Payload(BaseModel):
    source: str
    target: str
    text: str

@app.get("/health")
def health():
    import json, urllib.request
    base = BACKEND.rsplit("/",1)[0] if BACKEND.endswith("/translate") else BACKEND
    ok=False
    try:
        with urllib.request.urlopen(urllib.request.Request(f"{base}/health", headers={"Connection":"close"}), timeout=3) as r:
            j=

cd ~/anni/models && . .venv/bin/activate
python - <<'PY'
import sys, pkgutil
print("py", sys.version)
print("torch", bool(pkgutil.find_loader("torch")))
print("transformers", bool(pkgutil.find_loader("transformers")))
print("sentencepiece", bool(pkgutil.find_loader("sentencepiece")))
